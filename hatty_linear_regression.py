# -*- coding: utf-8 -*-
"""hatty_linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fgjDQ6hCScbsCWsRSOZsg8OWmQuqUCbH
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib import rc
import unittest
import re
import datetime

# %matplotlib inline

sns.set(style='whitegrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 14, 8

pd.options.display.float_format = '{:,.2f}'.format

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)

def run_tests():
  unittest.main(argv=[''], verbosity=1, exit=False)

"""# Load the data

Data [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
"""

!wget https://raw.githubusercontent.com/natecostello/linear-regression/master/sold.csv
!wget https://raw.githubusercontent.com/natecostello/linear-regression/master/for-sale.csv

df_train = pd.read_csv('sold.csv')
df_sale = pd.read_csv('for-sale.csv')

"""## Format and parse the data

The CSV produced by the scraper spits out a lot of fields we don't need.  Delete the following columns: "web-scraper-order", "web-scraper-start-url", "price", "PaginationLinks", and "PaginationLinks-href".

This leaves the following useful fields:
* ListingLink-href
* YearBuilt	
* LotSize	
* PropertyDetails	
* beds	
* baths	
* size	
* date-sold	
* address	
* price-link

"""

df_train.drop('web-scraper-order', inplace=True, axis=1)
df_train.drop('web-scraper-start-url', inplace=True, axis=1)
df_train.drop('price', inplace=True, axis=1)
df_train.drop('PaginationLinks', inplace=True, axis=1)
df_train.drop('PaginationLinks-href', inplace=True, axis=1)

df_sale.drop('web-scraper-order', inplace=True, axis=1)
df_sale.drop('web-scraper-start-url', inplace=True, axis=1)
df_sale.drop('price', inplace=True, axis=1)
df_sale.drop('PaginationLinks', inplace=True, axis=1)
df_sale.drop('PaginationLinks-href', inplace=True, axis=1)

"""The scraper also spits out alot of entries with missing data.  Usually this means the entire entry is suspect.   Delete any rows where the "beds", "baths", "size", "price-link", or "address" values are blank, "null", or "No Data".

"""

df_train.dropna(subset=['beds'], inplace=True)
df_train.dropna(subset=['baths'], inplace=True)
df_train.dropna(subset=['size'], inplace=True)
df_train.dropna(subset=['address'], inplace=True)
df_train.dropna(subset=['price-link'], inplace=True)

df_sale.dropna(subset=['beds'], inplace=True)
df_sale.dropna(subset=['baths'], inplace=True)
df_sale.dropna(subset=['size'], inplace=True)
df_sale.dropna(subset=['address'], inplace=True)
df_sale.dropna(subset=['price-link'], inplace=True)

"""
In many cases, the same property is listed with a different city and zip or with "LOT X" added to the street address.  To aid with de-duplication, Break the "address" field into "StreetAddress", "Street", "City", "State", "Zip".
"""

df_train['StreetAddress'] = df_train['address'].str.split(',').str[0]
df_train['City'] = df_train['address'].str.split(',').str[1]
df_train['State'] = df_train['address'].str.split(',').str[2].str.split(' ').str[1]
df_train['Zip'] = df_train['address'].str.split(',').str[2].str.split(' ').str[2]

df_sale['StreetAddress'] = df_sale['address'].str.split(',').str[0]
df_sale['City'] = df_sale['address'].str.split(',').str[1]
df_sale['State'] = df_sale['address'].str.split(',').str[2].str.split(' ').str[1]
df_sale['Zip'] = df_sale['address'].str.split(',').str[2].str.split(' ').str[2]

"""
For each row's "StreetAddress" field, delete text that matches "LOT X" where X represents any digit, digits, letter, or hyphenated combination.

With this done, we can drop the "address" field."""

target_regex = '\ (L|l)(O|o)(T|t)\ (([0-9]+-*[0-9]*[A-Z]*)|[A-Z])'
df_train['StreetAddress'] = df_train['StreetAddress'].str.replace(target_regex, '')
df_train.drop('address', inplace=True, axis=1)

df_sale['StreetAddress'] = df_sale['StreetAddress'].str.replace(target_regex, '')
df_sale.drop('address', inplace=True, axis=1)

"""For Lotsize we need to standardize on one unit since some entrees are in sqft and some are in acres."""

def convertToAcres(x):
  x = str(x).replace(',', '')
  if "No Data" in str(x):
    return 0.0
  if " Acres" in str(x):
    return float(str(x).replace(' Acres', ''))
  else:
    if " sqft" in str(x):
      sqft_val = float(str(x).replace(' sqft', ''))
      return sqft_val/43560
  return 0.0

df_train['LotSize'] = df_train['LotSize'].apply(convertToAcres)

df_sale['LotSize'] = df_sale['LotSize'].apply(convertToAcres)

"""For YearBuilt we need to remove the occasional "Built in"."""

def extractYearBuilt(x):
  target_regex = '[0-9]+'
  match = re.search(target_regex,str(x))
  if match:
    return int(match[0])
  else:
    return 0

df_train['YearBuilt'] = df_train['YearBuilt'].apply(extractYearBuilt)

df_sale['YearBuilt'] = df_sale['YearBuilt'].apply(extractYearBuilt)

"""
For each rows with duplicate "StreetAddress":
Take the higher of all numerical fields, take the city and zip that match cities being evaluated, and merge the PropertyDetails field.

Helpful stackoverflow:
https://stackoverflow.com/questions/36271413/pandas-merge-nearly-duplicate-rows-based-on-column-value

ListingLink-href
YearBuilt
LotSize
PropertyDetails
beds
baths
size
date-sold
address
price-link"""

df_train = df_train.groupby('StreetAddress').agg({'ListingLink-href' : 'first',
                                                  'YearBuilt' : 'max',
                                                  'LotSize' : 'max',
                                                  'beds' : 'max',
                                                  'baths' : 'max',
                                                  'size' : 'max',
                                                  'date-sold':'first',
                                                  'price-link' : 'first',
                                                  'PropertyDetails' : ', '.join}).reset_index()

df_sale = df_sale.groupby('StreetAddress').agg({'ListingLink-href' : 'first',
                                                  'YearBuilt' : 'max',
                                                  'LotSize' : 'max',
                                                  'beds' : 'max',
                                                  'baths' : 'max',
                                                  'size' : 'max',
                                                  'price-link' : 'first',
                                                  'PropertyDetails' : ', '.join}).reset_index()

"""We now need to add binaries based on whether the "PropertyDescription" lists the property as beach front, sound front, or canal front.

"""

def hasPrivatePool(x):
  target_regex = 'Private pool: Yes'
  if re.search(target_regex, str(x), re.IGNORECASE):
    return 1
  else:
    return 0

df_train['PrivatePool'] = df_train['PropertyDetails'].apply(hasPrivatePool)
df_sale['PrivatePool'] = df_sale['PropertyDetails'].apply(hasPrivatePool)


def isOceanFront(x):
  target_regex = '(waterfront features: ocean front)|(waterfront features: oceanfront)'
  if re.search(target_regex, str(x), re.IGNORECASE):
    return 1
  else:
    return 0

df_train['OceanFront'] = df_train['PropertyDetails'].apply(isOceanFront)
df_sale['OceanFront'] = df_sale['PropertyDetails'].apply(isOceanFront)

def isSoundFront(x):
  target_regex = '(waterfront features: sound front)|(waterfront features: soundfront)'
  if re.search(target_regex, str(x), re.IGNORECASE):
    return 1
  else:
    return 0

df_train['SoundFront'] = df_train['PropertyDetails'].apply(isSoundFront)
df_sale['SoundFront'] = df_sale['PropertyDetails'].apply(isSoundFront)

def isCanalFront(x):
  target_regex = '(waterfront features: canal front)|(waterfront features: canalfront)'
  if re.search(target_regex, str(x), re.IGNORECASE):
    return 1
  else:
    return 0

df_train['CanalFront'] = df_train['PropertyDetails'].apply(isCanalFront)
df_sale['CanalFront'] = df_sale['PropertyDetails'].apply(isCanalFront)

def hasBeachAccess(x):
  target_regex = '(beach access)'
  if re.search(target_regex, str(x), re.IGNORECASE):
    return 1
  else:
    return 0

df_train['BeachAccess'] = df_train['PropertyDetails'].apply(hasBeachAccess)
df_sale['BeachAccess'] = df_sale['PropertyDetails'].apply(hasBeachAccess)

def hasWaterview(x):
  target_regex = '(has waterview: yes)'
  if re.search(target_regex, str(x), re.IGNORECASE):
    return 1
  else:
    return 0

df_train['WaterView'] = df_train['PropertyDetails'].apply(hasWaterview)
df_sale['WaterView'] = df_sale['PropertyDetails'].apply(hasWaterview)

def lotsFromOceanFront(x):
  target_regex = '(lots from oceanfront)'
  if re.search(target_regex, str(x), re.IGNORECASE):
    return 1
  else:
    return 0

df_train['LotsFromOceanFront'] = df_train['PropertyDetails'].apply(lotsFromOceanFront)
df_sale['LotsFromOceanFront'] = df_sale['PropertyDetails'].apply(lotsFromOceanFront)

"""Finally, we just need to rename some columns to match legacy code."""

df_train.rename(columns={"price-link" : "Price", "size" : "Size", "beds" : "Bedrooms", "baths" : "Bathrooms", "date-sold" : "DateSold" }, inplace=True)
df_sale.rename(columns={"price-link" : "Price", "size" : "Size", "beds" : "Bedrooms", "baths" : "Bathrooms" }, inplace=True)

df_train['Price'] = df_train['Price'].replace({'\$': '', ',': ''}, regex=True).astype(float)
df_train['Size'] = df_train['Size'].replace({',': ''}, regex=True).astype(float)

df_sale['Price'] = df_sale['Price'].replace({'\$': '', ',': ''}, regex=True).astype(float)
df_sale['Size'] = df_sale['Size'].replace({',': ''}, regex=True).astype(float)


df_train['DateSold'] = pd.to_datetime(df_train['DateSold'])

df_train['MonthsSold'] = datetime.datetime.now() - df_train['DateSold']
df_train['MonthsSold'] = df_train['MonthsSold']/np.timedelta64(1,'M')
#df_train['MonthsSold'] = pd.to_numeric(df_train['MonthsSold'])
df_train.dtypes

"""# Data exploration"""

print(df_train['Price'].describe())
print(df_train['Size'].describe())
print(df_train['SoundFront'].describe())
print(df_train['OceanFront'].describe())
print(df_train['CanalFront'].describe())
print(df_train['BeachAccess'].describe())
print(df_train['WaterView'].describe())
print(df_train['LotsFromOceanFront'].describe())
print(df_train['DateSold'])
print(df_train['MonthsSold'])

sns.distplot(df_train['Price']);

sns.distplot(df_train['MonthsSold']);

var = 'Size'
data = pd.concat([df_train['Price'], df_train[var]], axis=1)
#data.plot.scatter(x=var, y='Price', ylim=(0,2500000), s=32);
data.plot.scatter(x=var, y='Price', s=32);
#print(data)

var = 'LotSize'
data = pd.concat([df_train['Price'], df_train[var]], axis=1)
data.plot.scatter(x=var, y='Price', ylim=(0,2500000));

var = 'OceanFront'
data = pd.concat([df_train['Price'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(14, 8))
fig = sns.boxplot(x=var, y="Price", data=data)
fig.axis(ymin=0, ymax=2500000);

var = 'PrivatePool'
data = pd.concat([df_train['Price'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(14, 8))
fig = sns.boxplot(x=var, y="Price", data=data)
fig.axis(ymin=0, ymax=2500000);

var = 'BeachAccess'
data = pd.concat([df_train['Price'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(14, 8))
fig = sns.boxplot(x=var, y="Price", data=data)
fig.axis(ymin=0, ymax=2500000);

var = 'Bedrooms'
data = pd.concat([df_train['Price'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(14, 8))
fig = sns.boxplot(x=var, y="Price", data=data)
fig.axis(ymin=0, ymax=2500000);

corrmat = df_train.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

k = 12 #number of variables for heatmap
cols = corrmat.nlargest(k, 'Price')['Price'].index
f, ax = plt.subplots(figsize=(14, 10))
sns.heatmap(df_train[cols].corr(), vmax=.8, square=True);

cols = ['Price', 'Size', 'Bathrooms', 'Bedrooms']
sns.pairplot(df_train[cols], height = 4);

"""## Do we have missing data?"""

total = df_train.isnull().sum().sort_values(ascending=False)
percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)

"""# Predicting the sale price

## Preparing the data

### Feature scaling

We will do a little preprocessing to our data using the following formula (standardization):

$$x'= \frac{x - \mu}{\sigma}$$

where $\mu$ is the population mean and $\sigma$ is the standard deviation.

![](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/FeatureScaling.jpg)

**Source: Andrew Ng**
"""

x = df_train['Size']
y = df_train['Price']

x = (x - x.mean()) / x.std()
x = np.c_[np.ones(x.shape[0]), x]

x.shape

y[0]

x

"""## Linear Regression

![](https://i.ytimg.com/vi/zPG4NjIkCjc/maxresdefault.jpg)

**Source: MyBookSucks**

Linear regression models assume that the relationship between a dependent continuous variable $Y$ and one or more explanatory (independent) variables $X$ is linear (that is, a straight line). It‚Äôs used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). Linear regression models can be divided into two main types:

### Simple Linear Regression

Simple linear regression uses a traditional slope-intercept form, where $a$ and $b$ are the coefficients that we try to ‚Äúlearn‚Äù and produce the most accurate predictions. $X$ represents our input data and $Y$ is our prediction.

$$Y = bX + a$$

![](https://spss-tutorials.com/img/simple-linear-regression-equation-linear-relation.png)

**Source: SPSS tutorials**

### Multivariable Regression

A more complex, multi-variable linear equation might look like this, where w represents the coefficients, or weights, our model will try to learn.

$$ Y(x_1,x_2,x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_0$$

The variables $x_1, x_2, x_3$ represent the attributes, or distinct pieces of information, we have about each observation.

## Loss function

Given our Simple Linear Regression equation:

$$Y = bX + a$$

We can use the following cost function to find the coefficients:

### Mean Squared Error (MSE) Cost Function

The MSE is defined as:

$$MSE = J(W) =  \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - h_w(x^{(i)}))^2$$

where

$$h_w(x) = g(w^Tx)$$

The MSE measures how much the average model predictions vary from the correct values. The number is higher when the model is performing "bad" on the training set.

The first derivative of MSE is given by:

$$MSE' = J'(W) = \frac{2}{m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})$$


### One Half Mean Squared Error (OHMSE)

We will apply a small modification to the MSE - multiply by $\frac{1}{2}$ so when we take the derivative, the `2`s cancel out:

$$ OHMSE = J(W) =  \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - h_w(x^{(i)}))^2 $$

The first derivative of OHMSE is given by:

$$OHMSE' = J'(W) = \frac{1}{m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})$$
"""

def loss(h, y):
  sq_error = (h - y)**2
  n = len(y)
  return 1.0 / (2*n) * sq_error.sum()

class TestLoss(unittest.TestCase):

  def test_zero_h_zero_y(self):
    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([0])), 0)

  def test_one_h_zero_y(self):
    self.assertAlmostEqual(loss(h=np.array([1]), y=np.array([0])), 0.5)

  def test_two_h_zero_y(self):
    self.assertAlmostEqual(loss(h=np.array([2]), y=np.array([0])), 2)
    
  def test_zero_h_one_y(self):
    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([1])), 0.5)
    
  def test_zero_h_two_y(self):
    self.assertAlmostEqual(loss(h=np.array([0]), y=np.array([2])), 2)

run_tests()

class LinearRegression:
  
  def predict(self, X):
    return np.dot(X, self._W)
  
  def _gradient_descent_step(self, X, targets, lr):

    predictions = self.predict(X)
    
    error = predictions - targets
    gradient = np.dot(X.T,  error) / len(X)
    
    self._W -= lr * gradient
      
  def fit(self, X, y, n_iter=100000, lr=0.01):

    self._W = np.zeros(X.shape[1])

    self._cost_history = []
    self._w_history = [self._W]
    for i in range(n_iter):
      
        prediction = self.predict(X)
        cost = loss(prediction, y)
        
        self._cost_history.append(cost)
        
        self._gradient_descent_step(x, y, lr)
        
        self._w_history.append(self._W.copy())
    return self

class TestLinearRegression(unittest.TestCase):

    def test_find_coefficients(self):
      clf = LinearRegression()
      clf.fit(x, y, n_iter=2000, lr=0.01)
      np.testing.assert_array_almost_equal(clf._W, np.array([180921.19555322,  56294.90199925]))

clf = LinearRegression()
clf.fit(x, y, n_iter=2000, lr=0.01)

run_tests()

clf = LinearRegression()
clf.fit(x, y, n_iter=2000, lr=0.01)

clf._W

plt.title('Cost Function J')
plt.xlabel('No. of iterations')
plt.ylabel('Cost')
plt.plot(clf._cost_history)
plt.show()

clf._cost_history[-1]

#Animation

#Set the plot up,
fig = plt.figure()
ax = plt.axes()
plt.title('Sale Price vs Living Area')
plt.xlabel('Living Area in square feet (normalised)')
plt.ylabel('Sale Price ($)')
plt.scatter(x[:,1], y)
line, = ax.plot([], [], lw=2, color='red')
annotation = ax.text(-1, 700000, '')
annotation.set_animated(True)
plt.close()

#Generate the animation data,
def init():
    line.set_data([], [])
    annotation.set_text('')
    return line, annotation

# animation function.  This is called sequentially
def animate(i):
    x = np.linspace(-5, 20, 1000)
    y = clf._w_history[i][1]*x + clf._w_history[i][0]
    line.set_data(x, y)
    annotation.set_text('Cost = %.2f e10' % (clf._cost_history[i]/10000000000))
    return line, annotation

anim = animation.FuncAnimation(fig, animate, init_func=init,
                               frames=300, interval=10, blit=True)

rc('animation', html='jshtml')

anim

"""# Multivariable Linear Regression

Let's use more of the available data to build a Multivariable Linear Regression model and see whether or not that will improve our OHMSE error:
"""

x = df_train[['Bedrooms', 'Bathrooms', 'Size', 'PrivatePool', 'OceanFront', 'BeachAccess', 'WaterView']]

x = (x - x.mean()) / x.std()
x = np.c_[np.ones(x.shape[0]), x] 

clf = LinearRegression()
clf.fit(x, y, n_iter=2000, lr=0.01)

clf._W

plt.title('Cost Function J')
plt.xlabel('No. of iterations')
plt.ylabel('Cost')
plt.plot(clf._cost_history)
plt.show()

clf._cost_history[-1]

print("['Bedrooms', 'Bathrooms', 'Size', 'PrivatePool', 'Oceanfront', 'Soundfront']")
clf._W

"""Now, calculate some values:

ùëå(ùë•1,ùë•2,ùë•3)=ùë§1ùë•1+ùë§2ùë•2+ùë§3ùë•3+ùë§0
"""

x = df_train[['Bedrooms', 'Bathrooms', 'Size', 'PrivatePool', 'OceanFront', 'BeachAccess', 'WaterView']]
data = df_sale[['Bedrooms', 'Bathrooms', 'Size', 'PrivatePool', 'OceanFront', 'BeachAccess', 'WaterView']]
#data_columns = ['Bedrooms', 'Bathrooms', 'Size', 'PrivatePool', 'OceanFront', 'BeachAccess', 'WaterView']
#data_vals = [[4, 2, 1527, 0, 0, 1, 1]]
#data_vals.append([4,3,1766, 0, 1, 1, 1])
#data_df = pd.DataFrame(data_vals, columns=data_columns)
data = (data - x.mean()) / x.std()
data = np.c_[np.ones(data.shape[0]), data] 

y = data.dot(clf._W)
df_sale['ModelPrice'] = y
df_sale['ModelPrice'].describe()
df_sale['ModelDelta'] = df_sale['Price'] - df_sale['ModelPrice']

#print(df_sale[['Price', 'ModelPrice', 'ModelDelta', 'ListingLink-href']])
df_sale[['Price', 'ModelPrice', 'ModelDelta', 'Bedrooms' 'ListingLink-href']].to_csv()